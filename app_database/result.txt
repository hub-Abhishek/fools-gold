{'query': 'query', 'result': 'a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d', 'source_documents': [Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='the sequence\nlengthnis smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\n6', metadata={'title': '1706.03762.pdf', 'page_number': 5}), Document(page_content='Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difﬁcult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\n13', metadata={'title': '1706.03762.pdf', 'page_number': 12}), Document(page_content='size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\n5', metadata={'title': '1706.03762.pdf', 'page_number': 4})]}/n/n{'query': 'query', 'result': 'a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d-input if a d', 'source_documents': [Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='the sequence\nlengthnis smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\n6', metadata={'title': '1706.03762.pdf', 'page_number': 5}), Document(page_content='Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb ‘making’, completing the phrase ‘making...more difﬁcult’. Attentions here shown only for\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\n13', metadata={'title': '1706.03762.pdf', 'page_number': 12}), Document(page_content='size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\n5', metadata={'title': '1706.03762.pdf', 'page_number': 4})]}/n/n{'query': 'query', 'result': 'a set of key-value pairs to an output', 'source_documents': [Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\n5', metadata={'title': '1706.03762.pdf', 'page_number': 4}), Document(page_content='into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\x01k=Pdk\ni=1qiki, has mean 0and variance dk.\n4', metadata={'title': '1706.03762.pdf', 'page_number': 3}), Document(page_content='to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3', metadata={'title': '1706.03762.pdf', 'page_number': 2})]}/n/n{'query': 'query', 'result': 'Query:', 'source_documents': [Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\n5', metadata={'title': '1706.03762.pdf', 'page_number': 4}), Document(page_content='size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\n5', metadata={'title': '1706.03762.pdf', 'page_number': 4})]}/n/n{'query': 'query', 'result': 'Query:', 'source_documents': [Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\n5', metadata={'title': '1706.03762.pdf', 'page_number': 4})]}/n/n{'query': 'my name is abhishek. what is this paper about?', 'result': 'Embeddings and Softmax', 'source_documents': [Document(page_content='my name is abhishek. what is this paper about?', metadata={'title': 'query(ies)', 'query_number': 1}), Document(page_content='size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\n5', metadata={'title': '1706.03762.pdf', 'page_number': 4}), Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\x01k=Pdk\ni=1qiki, has mean 0and variance dk.\n4', metadata={'title': '1706.03762.pdf', 'page_number': 3})]}/n/n{'query': 'what is my name?', 'result': 'input when generating the next', 'source_documents': [Document(page_content='what is my name?', metadata={'title': 'query(ies)', 'query_number': 1}), Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\n5', metadata={'title': '1706.03762.pdf', 'page_number': 4}), Document(page_content='into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\x01k=Pdk\ni=1qiki, has mean 0and variance dk.\n4', metadata={'title': '1706.03762.pdf', 'page_number': 3})]}/n/n{'query': 'what is my name?', 'result': 'ada', 'source_documents': [Document(page_content='what is my name?', metadata={'title': 'query(ies)', 'query_number': 1})]}/n/n{'query': 'my name is abhishek', 'result': 'a good friend', 'source_documents': [Document(page_content='my name is abhishek', metadata={'title': 'query(ies)', 'query_number': 1})]}/n/n{'query': 'what is my name', 'result': 'adolf', 'source_documents': [Document(page_content='what is my name', metadata={'title': 'query(ies)', 'query_number': 1})]}/n/n{'query': 'what is my name?', 'result': 'ada', 'source_documents': [Document(page_content='what is my name?', metadata={'title': 'query(ies)', 'query_number': 1})]}/n/n{'query': 'what is my name?', 'result': 'ada', 'source_documents': [Document(page_content='what is my name?', metadata={'title': 'query(ies)', 'query_number': 1})]}/n/n{'query': 'what is my name?', 'result': 'ada', 'source_documents': [Document(page_content='what is my name?', metadata={'title': 'query(ies)', 'query_number': 1})]}/n/n{'query': 'what is my name?', 'result': 'ada', 'source_documents': [Document(page_content='what is my name?', metadata={'title': 'query(ies)', 'query_number': 1})]}/n/n{'query': 'what is my name?', 'result': 'ada', 'source_documents': [Document(page_content='what is my name?', metadata={'title': 'query(ies)', 'query_number': 1})]}/n/n{'query': 'my name is abhishek.', 'result': "I'm not sure what to say.", 'source_documents': [Document(page_content='my name is abhishek.', metadata={'title': 'query(ies)', 'query_number': 1}), Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\n5', metadata={'title': '1706.03762.pdf', 'page_number': 4}), Document(page_content='into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\x01k=Pdk\ni=1qiki, has mean 0and variance dk.\n4', metadata={'title': '1706.03762.pdf', 'page_number': 3})]}/n/n{'query': 'what is my name?', 'result': 'input when generating the next', 'source_documents': [Document(page_content='what is my name?', metadata={'title': 'query(ies)', 'query_number': 1}), Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\n5', metadata={'title': '1706.03762.pdf', 'page_number': 4}), Document(page_content='into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\x01k=Pdk\ni=1qiki, has mean 0and variance dk.\n4', metadata={'title': '1706.03762.pdf', 'page_number': 3})]}/n/n{'query': 'my name is abhishek.  who are the authors of this paper?', 'result': 'ahsa', 'source_documents': [Document(page_content='my name is abhishek.  who are the authors of this paper?', metadata={'title': 'query(ies)', 'query_number': 1}), Document(page_content='size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\n5', metadata={'title': '1706.03762.pdf', 'page_number': 4}), Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='time, the number of GPUs used, and an estimate of the sustained\nsingle-precision ﬂoating-point capacity of each GPU5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8', metadata={'title': '1706.03762.pdf', 'page_number': 7})]}/n/n{'query': 'what is my name?', 'result': 'abhishek', 'source_documents': [Document(page_content='my name is abhishek.  who are the authors of this paper?  what is my name?', metadata={'title': 'query(ies)', 'query_number': 1}), Document(page_content='input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n2', metadata={'title': '1706.03762.pdf', 'page_number': 1}), Document(page_content='size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights bypdmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\n5', metadata={'title': '1706.03762.pdf', 'page_number': 4}), Document(page_content='into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\x01k=Pdk\ni=1qiki, has mean 0and variance dk.\n4', metadata={'title': '1706.03762.pdf', 'page_number': 3})]}/n/n{'query': 'query', 'result': 'a snorting dog', 'source_documents': [Document(page_content='query', metadata={'title': 'query(ies)', 'query_number': 1})]}/n/n{'query': 'query', 'result': '', 'source_documents': [Document(page_content='128063000\n2009: 128047000\n2010: 128070000\n2011: 127833000\n2012: 127629000\n2013: 127445000\n2014: 127276000\n2015: 127141000\n2016: 127076000\n2017: 126972000\n2018: 126811000\n2019: 126633000\n2020: 126261000\n2021: 125681593\n2022:', metadata={'source': 'C:\\Users\\abhis\\AppData\\Local\\Temp\\tmpfg9hcxl_', 'row': 119}), Document(page_content='1311020000\n2007: 1317885000\n2008: 1324655000\n2009: 1331260000\n2010: 1337705000\n2011: 1345035000\n2012: 1354190000\n2013: 1363240000\n2014: 1371860000\n2015: 1379860000\n2016: 1387790000\n2017: 1396215000\n2018: 1402760000\n2019: 1407745000\n2020: 1411100000\n2021: 1412360000\n2022:', metadata={'source': 'C:\\Users\\abhis\\AppData\\Local\\Temp\\tmpfg9hcxl_', 'row': 40}), Document(page_content='190123222\n2010: 194454498\n2011: 198602738\n2012: 202205861\n2013: 205337562\n2014: 208251628\n2015: 210969298\n2016: 213524840\n2017: 216379655\n2018: 219731479\n2019: 223293280\n2020: 227196741\n2021: 231402117\n2022:', metadata={'source': 'C:\\Users\\abhis\\AppData\\Local\\Temp\\tmpfg9hcxl_', 'row': 184}), Document(page_content='146706810\n2010: 148391139\n2011: 150211005\n2012: 152090649\n2013: 154030139\n2014: 155961299\n2015: 157830000\n2016: 159784568\n2017: 161793964\n2018: 163683958\n2019: 165516222\n2020: 167420951\n2021: 169356251\n2022:', metadata={'source': 'C:\\Users\\abhis\\AppData\\Local\\Temp\\tmpfg9hcxl_', 'row': 20})]}/n/n{'query': 'query', 'result': 'a', 'source_documents': []}/n/n